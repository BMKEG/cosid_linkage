{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a list of open access PMID files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import call\n",
    "\n",
    "pmids = []\n",
    "pmid_file = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/pmids.txt\"\n",
    "\n",
    "with open(pmid_file) as f:\n",
    "    for line in f.readlines():\n",
    "        pmids.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Shell commands to build the zipped bundle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# echo -n application/vnd.wf4ever.robundle+zip > mimetype\n",
    "# zip -0 -X ../reach mimetype\n",
    "# zip -X -r ../reach . -x mimetype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sentences from each paper processed by SciDT into simple sentences for each Figure assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def retrieve_sentences_for_modeling(inFile, fid):\n",
    "    \n",
    "    tsv = pd.read_csv(inFile, sep='\\t')\n",
    "    fig_tagged_sentences = {}\n",
    "\n",
    "    for i, row in tsv.iterrows():\n",
    "        sid = row['SentenceId']\n",
    "        codeStr = row['Codes']\n",
    "        paragraph = row['Paragraph']\n",
    "        text = row['Sentence Text']\n",
    "        heading = row['Headings']\n",
    "        floatingBox = row['FloatingBox?']\n",
    "        discourse = row['Discourse Type']\n",
    "        reachData = row['friesEventsTypes']\n",
    "        fig = row['Figure Assignment']\n",
    "        offset_start = row['Offset_Begin']\n",
    "        offset_end = row['Offset_End']\n",
    "        \n",
    "        if fig == fig:\n",
    "            for f in fig.split('|'):\n",
    "                if( fig_tagged_sentences.get(f, None) is None ):\n",
    "                    sent_list = []\n",
    "                    fig_tagged_sentences[f] = sent_list\n",
    "                    sent_list.append({'sid': sid, 'pid':paragraph, \n",
    "                                    'start': offset_start, 'end': offset_end, 'text': text,\n",
    "                                    'discourse_types': discourse})\n",
    "                else:\n",
    "                    sent_list = fig_tagged_sentences[f]\n",
    "                    sent_list.append({'sid': sid, 'pid':paragraph, \n",
    "                                    'start': offset_start, 'end': offset_end, 'text': text,\n",
    "                                    'discourse_types': discourse})\n",
    "                \n",
    "            \n",
    "    return fig_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10087260\n",
      "10087263\n",
      "10087265\n",
      "10209036\n",
      "10225955\n",
      "10366597\n",
      "10366599\n",
      "10385523\n",
      "10385526\n",
      "10402465\n",
      "10429675\n",
      "10545507\n",
      "10562275\n",
      "10562277\n",
      "10562279\n",
      "10562288\n",
      "10601328\n",
      "10601346\n",
      "10613896\n",
      "10620603\n",
      "10629222\n",
      "10648568\n",
      "10662770\n",
      "10684247\n",
      "10704439\n",
      "10704444\n",
      "10704446\n",
      "10725331\n",
      "10725334\n",
      "10747088\n",
      "10747089\n",
      "10790433\n",
      "10811823\n",
      "10831611\n",
      "10859335\n",
      "10864201\n",
      "10871282\n",
      "10900456\n",
      "10931856\n",
      "10931877\n",
      "10953014\n",
      "10974003\n",
      "10995436\n",
      "11018051\n",
      "11018064\n",
      "11034606\n",
      "11038172\n",
      "11038182\n",
      "11076969\n",
      "11086001\n",
      "11134073\n",
      "11149930\n",
      "11157975\n",
      "11157979\n",
      "11157984\n",
      "11181702\n",
      "11257119\n",
      "11266443\n",
      "11266449\n",
      "11266451\n",
      "11309418\n",
      "11401320\n",
      "11402059\n",
      "11448995\n",
      "11502761\n",
      "11514608\n",
      "11564755\n",
      "11570975\n",
      "11571312\n",
      "11591728\n",
      "11591731\n",
      "11684708\n",
      "11724822\n",
      "11739401\n",
      "11739402\n",
      "11739404\n",
      "11747467\n",
      "11756480\n",
      "11777938\n",
      "11846885\n",
      "11877480\n",
      "11914126\n",
      "11916981\n",
      "11927603\n",
      "11927608\n",
      "12011112\n",
      "12147674\n",
      "12167173\n",
      "12199906\n",
      "12421467\n",
      "12446742\n",
      "12473693\n",
      "12486103\n",
      "12486115\n",
      "12507995\n",
      "12527750\n",
      "12566426\n",
      "12642614\n",
      "12682088\n",
      "12689351\n",
      "12719471\n",
      "12771128\n",
      "12782684\n",
      "12847081\n",
      "12900395\n",
      "12939254\n",
      "14517205\n",
      "14568990\n",
      "14612908\n",
      "14638857\n",
      "14707117\n",
      "14709540\n",
      "14734533\n",
      "14737190\n",
      "14970179\n",
      "15037601\n",
      "15045029\n",
      "15051809\n",
      "15070402\n",
      "15078903\n",
      "15096524\n",
      "15148308\n",
      "15159416\n",
      "15302858\n",
      "15314064\n",
      "15326198\n",
      "1541635\n",
      "15477347\n",
      "15504911\n",
      "15575970\n",
      "15583694\n",
      "15642746\n",
      "15653635\n",
      "15720729\n",
      "15767459\n",
      "15796781\n",
      "15828860\n",
      "15883195\n",
      "15928207\n",
      "15967037\n",
      "16000169\n",
      "16027220\n",
      "16043514\n",
      "16043515\n",
      "16061695\n",
      "16087707\n",
      "16098226\n",
      "16115959\n",
      "16166655\n",
      "16179646\n",
      "16179649\n",
      "16203867\n",
      "16254079\n",
      "16301747\n",
      "16356270\n",
      "16396833\n",
      "16396834\n",
      "16403219\n",
      "16415179\n",
      "16449187\n",
      "16492808\n",
      "16513846\n",
      "16520382\n",
      "16545136\n",
      "16603075\n",
      "16606443\n",
      "16618814\n",
      "16636147\n",
      "16638120\n",
      "16672054\n",
      "16717130\n",
      "16729043\n",
      "16754960\n",
      "16839418\n",
      "16847100\n",
      "16872538\n",
      "16880273\n",
      "16893970\n",
      "16923827\n",
      "16945160\n",
      "16982639\n",
      "16990252\n",
      "17000877\n",
      "17030985\n",
      "17085477\n",
      "17112379\n",
      "17151076\n",
      "17183697\n",
      "17224084\n",
      "17280616\n",
      "17284314\n",
      "17341466\n",
      "17353368\n",
      "17353931\n",
      "17407569\n",
      "17412707\n",
      "17470632\n",
      "17485491\n",
      "17485524\n",
      "17500595\n",
      "17511879\n",
      "17543119\n",
      "17557078\n",
      "17581628\n",
      "17591856\n",
      "17605817\n",
      "17608567\n",
      "17612402\n",
      "17620405\n",
      "17620407\n",
      "17623094\n",
      "17627824\n",
      "17650322\n",
      "17660750\n",
      "17660751\n",
      "17667950\n",
      "17690686\n",
      "17721441\n",
      "17724128\n",
      "17762866\n",
      "17889823\n",
      "17936057\n",
      "17937504\n",
      "17948060\n",
      "17986458\n",
      "18000013\n",
      "18034155\n",
      "18086859\n",
      "18154663\n",
      "18157088\n",
      "18171471\n",
      "18188153\n",
      "18188154\n",
      "18208323\n",
      "18226242\n",
      "18239682\n",
      "18256700\n",
      "18266467\n",
      "18268103\n",
      "18286207\n",
      "18289379\n",
      "18292755\n",
      "18301737\n",
      "18309292\n",
      "18309293\n",
      "18309296\n",
      "18320063\n",
      "18354501\n",
      "18377662\n",
      "18388858\n",
      "18394558\n",
      "18412956\n",
      "18421166\n",
      "18430226\n",
      "18433452\n",
      "18435708\n",
      "18447585\n",
      "18452624\n",
      "18458160\n",
      "18466225\n",
      "18479511\n",
      "18497748\n",
      "18498651\n",
      "18498752\n",
      "18509523\n",
      "18511940\n",
      "18518979\n",
      "18551167\n",
      "18560762\n",
      "18573912\n",
      "18583960\n",
      "18583988\n",
      "18586827\n",
      "18604270\n",
      "18612383\n",
      "18617507\n",
      "18628297\n",
      "18628823\n",
      "18629017\n",
      "18631241\n",
      "18647389\n",
      "18662404\n",
      "18663010\n",
      "18665261\n",
      "18671868\n",
      "18682833\n",
      "18703495\n",
      "18724936\n",
      "18758438\n",
      "18761697\n",
      "18762578\n",
      "18762581\n",
      "18775314\n",
      "18775702\n",
      "18779372\n",
      "18781224\n",
      "18794331\n",
      "18800055\n",
      "18802460\n",
      "18808384\n",
      "18812399\n",
      "18818696\n",
      "18833289\n",
      "18836139\n",
      "18923419\n",
      "18927618\n",
      "18946488\n",
      "18953286\n",
      "18955484\n",
      "18985028\n",
      "19008859\n",
      "19019158\n",
      "19037259\n",
      "19055777\n",
      "19060904\n",
      "19063885\n",
      "19079254\n",
      "19088080\n",
      "19088272\n",
      "19107194\n",
      "19107203\n",
      "19114595\n",
      "19118384\n",
      "19131970\n",
      "19135240\n",
      "19150989\n",
      "19153600\n",
      "19155274\n",
      "19156129\n",
      "19158676\n",
      "19165350\n",
      "19167335\n",
      "19171758\n",
      "19214185\n",
      "19229298\n",
      "19277118\n",
      "19290556\n",
      "19322195\n",
      "19322197\n",
      "19329994\n",
      "19360002\n",
      "19369943\n",
      "19407811\n",
      "19419567\n",
      "19432797\n",
      "19440292\n",
      "19440376\n",
      "19455133\n",
      "19486527\n",
      "19494831\n",
      "19498462\n",
      "19498465\n",
      "19506933\n",
      "19520861\n",
      "19521502\n",
      "19523115\n",
      "19536134\n",
      "19536198\n",
      "19543227\n",
      "19567478\n",
      "19570034\n",
      "19570982\n",
      "19575010\n",
      "19590579\n",
      "19609305\n",
      "19616007\n",
      "19619546\n",
      "19625296\n",
      "19629177\n",
      "19635168\n",
      "19636380\n",
      "19648646\n",
      "19680228\n",
      "19680552\n",
      "19682256\n",
      "19690564\n",
      "19696784\n",
      "19701182\n",
      "19701191\n",
      "19704022\n",
      "19730435\n",
      "19730696\n",
      "19736317\n",
      "19746159\n",
      "19763081\n",
      "19765300\n",
      "19767740\n",
      "19781631\n",
      "19797078\n",
      "19798056\n",
      "19798101\n",
      "19807924\n",
      "19841731\n",
      "19874541\n",
      "19887001\n",
      "19888460\n",
      "19888464\n",
      "19893485\n",
      "19893486\n",
      "19893489\n",
      "19893491\n",
      "19903340\n",
      "19927124\n",
      "19933256\n",
      "19933576\n",
      "19934257\n",
      "19934264\n",
      "19940019\n",
      "19941819\n",
      "19941825\n",
      "19942852\n",
      "19959993\n",
      "19959995\n",
      "19996314\n",
      "20007317\n",
      "20037628\n",
      "20043912\n",
      "20071408\n",
      "20075079\n",
      "20075868\n",
      "20094031\n",
      "20098747\n",
      "20110348\n",
      "20111005\n",
      "20123736\n",
      "20129940\n",
      "20140193\n",
      "20141835\n",
      "20169075\n",
      "20169078\n",
      "20169165\n",
      "20174651\n",
      "20178605\n",
      "20186120\n",
      "20205919\n",
      "20211136\n",
      "20214800\n",
      "20224550\n",
      "20231380\n",
      "20300060\n",
      "20305656\n",
      "20308429\n",
      "20338032\n",
      "20353594\n",
      "20360680\n",
      "20362541\n",
      "20362542\n",
      "20368803\n",
      "20371544\n",
      "20375098\n",
      "20388642\n",
      "20399778\n",
      "20400938\n",
      "20410134\n",
      "20418871\n",
      "20418951\n",
      "20434988\n",
      "20436455\n",
      "20439537\n",
      "20453830\n",
      "20456499\n",
      "20467438\n",
      "20471980\n",
      "20512112\n",
      "20529865\n",
      "20540776\n",
      "20543819\n",
      "20559324\n",
      "20561531\n",
      "20562859\n",
      "20574810\n",
      "20579338\n",
      "20581830\n",
      "20584916\n",
      "20594350\n",
      "20601937\n",
      "20603002\n",
      "20603614\n",
      "20624308\n",
      "20628654\n",
      "20639901\n",
      "20639902\n",
      "20642453\n",
      "20657822\n",
      "20659021\n",
      "20664520\n",
      "20676093\n",
      "20676095\n",
      "20676135\n",
      "20686606\n",
      "20693977\n",
      "20697347\n",
      "20697357\n",
      "20700126\n",
      "20706207\n",
      "20711500\n",
      "20729920\n",
      "20738866\n",
      "20802085\n",
      "20802534\n",
      "20802536\n",
      "20817927\n",
      "20818336\n",
      "20818435\n",
      "20819940\n",
      "20840750\n",
      "20843328\n",
      "20856196\n",
      "20856200\n",
      "20856870\n",
      "20862261\n",
      "20865124\n",
      "20871633\n",
      "20881089\n",
      "20890303\n",
      "20890305\n",
      "20920251\n",
      "20924358\n",
      "20929568\n",
      "20929579\n",
      "20932347\n",
      "20935634\n",
      "20935647\n",
      "20935677\n",
      "20936779\n",
      "20953186\n",
      "20969766\n",
      "20972225\n",
      "20972459\n",
      "20976523\n",
      "21034468\n",
      "21037577\n",
      "21047798\n",
      "21048921\n",
      "21048939\n",
      "21057456\n",
      "21057510\n",
      "21063388\n",
      "21078624\n",
      "21092281\n",
      "21092292\n",
      "21110861\n",
      "21112398\n",
      "21113127\n",
      "21114864\n",
      "21118991\n",
      "21119599\n",
      "21119626\n",
      "21124868\n",
      "21124943\n",
      "21131964\n",
      "21131965\n",
      "21131967\n",
      "21132010\n",
      "21139566\n",
      "21147767\n",
      "21148288\n",
      "21149568\n",
      "21151104\n",
      "21157431\n",
      "2116421\n",
      "21170087\n",
      "21172016\n",
      "21179004\n",
      "21179020\n",
      "21179510\n",
      "2118142\n",
      "21186367\n",
      "21187329\n",
      "21203429\n",
      "21203436\n",
      "21209940\n",
      "21212461\n",
      "2121740\n",
      "21217644\n",
      "21217774\n",
      "21219645\n",
      "21220045\n",
      "21224849\n",
      "21224850\n",
      "21242965\n",
      "21242966\n",
      "21242980\n",
      "21245844\n",
      "21247419\n",
      "21251231\n",
      "21252856\n",
      "21274006\n",
      "21277013\n",
      "21278383\n",
      "21278420\n",
      "21278786\n",
      "21288885\n",
      "21297662\n",
      "21306563\n",
      "21311558\n",
      "21314951\n",
      "21317875\n",
      "21328542\n",
      "21335238\n",
      "21336258\n",
      "21338522\n",
      "21347350\n",
      "21364888\n",
      "21386817\n",
      "21386897\n",
      "21390248\n",
      "21399620\n",
      "21399639\n",
      "21399666\n",
      "21407176\n",
      "21407215\n",
      "21408167\n",
      "21415856\n",
      "21423209\n",
      "21423216\n",
      "21427704\n",
      "21439629\n",
      "21445305\n",
      "21447707\n",
      "21454693\n",
      "21498514\n",
      "21505799\n",
      "21507240\n",
      "21516116\n",
      "21525870\n",
      "21525958\n",
      "21526181\n",
      "21533037\n",
      "21541365\n",
      "21556049\n",
      "21559518\n",
      "21569246\n",
      "21575178\n",
      "21575199\n",
      "21577200\n",
      "21586138\n",
      "21602887\n",
      "21613545\n",
      "21625644\n",
      "21637789\n",
      "21642953\n",
      "21643011\n",
      "21668996\n",
      "21669201\n",
      "21679440\n",
      "21685908\n",
      "21685939\n",
      "21685944\n",
      "21689417\n",
      "21698133\n",
      "21701560\n",
      "21705390\n",
      "21706061\n",
      "21718540\n",
      "21725360\n",
      "21725367\n",
      "21734647\n",
      "21743437\n",
      "21743479\n",
      "21743491\n",
      "21747946\n",
      "21781306\n",
      "21798038\n",
      "21804533\n",
      "21811563\n",
      "21822214\n",
      "21847096\n",
      "21847100\n",
      "21857973\n",
      "21871133\n",
      "21874024\n",
      "21875956\n",
      "21880142\n",
      "21884581\n",
      "21890893\n",
      "21893585\n",
      "21903581\n",
      "21908610\n",
      "21909133\n",
      "21909281\n",
      "21931555\n",
      "21931591\n",
      "21943085\n",
      "21946559\n",
      "21946560\n",
      "21952049\n",
      "21964608\n",
      "21988832\n",
      "21998301\n",
      "22010978\n",
      "22014111\n",
      "22016384\n",
      "22022230\n",
      "22022540\n",
      "22027862\n",
      "22028648\n",
      "22034500\n",
      "22046270\n",
      "22048310\n",
      "22056778\n",
      "22056872\n",
      "22057290\n",
      "22059385\n",
      "22068330\n",
      "22072986\n",
      "22087277\n",
      "22094269\n",
      "22096563\n",
      "22102817\n",
      "22116401\n",
      "22118466\n",
      "22118625\n",
      "22135285\n",
      "22157895\n",
      "22162999\n",
      "22163275\n",
      "22174692\n",
      "22174833\n",
      "22207579\n",
      "22238662\n",
      "22242148\n",
      "22269274\n",
      "22270917\n",
      "22279592\n",
      "22280843\n",
      "22291595\n",
      "22303461\n",
      "22323290\n",
      "22323517\n",
      "22325148\n",
      "22334672\n",
      "22355679\n",
      "22363216\n",
      "22382979\n",
      "22387996\n",
      "22401567\n",
      "22402981\n",
      "22404908\n",
      "22406378\n",
      "22406686\n",
      "22413019\n",
      "22442151\n",
      "22446626\n",
      "22447027\n",
      "22458338\n",
      "22470507\n",
      "22471946\n",
      "22491013\n",
      "22493164\n",
      "22493500\n",
      "22500027\n",
      "22510880\n",
      "22510882\n",
      "22518138\n",
      "22540012\n",
      "22555292\n",
      "22575651\n",
      "22581261\n",
      "22609302\n",
      "22613832\n",
      "22623428\n",
      "22634751\n",
      "22648170\n",
      "22651821\n",
      "22662192\n",
      "22674187\n",
      "22685417\n",
      "22731636\n",
      "22791023\n",
      "22808155\n",
      "22829933\n",
      "22842785\n",
      "22850675\n",
      "22863774\n",
      "22892566\n",
      "22899650\n",
      "22904065\n",
      "22905162\n",
      "22908322\n",
      "22916011\n",
      "22939623\n",
      "22940692\n",
      "22952686\n",
      "22952718\n",
      "22962574\n",
      "22962849\n",
      "22966907\n",
      "22977175\n",
      "23022564\n",
      "23023393\n",
      "23042150\n",
      "23056421\n",
      "23065768\n",
      "23075850\n",
      "23082202\n",
      "23082758\n",
      "23085988\n",
      "23086447\n",
      "23086448\n",
      "23088713\n",
      "23104095\n",
      "23104097\n",
      "23142775\n",
      "23143267\n",
      "23161686\n",
      "23170778\n",
      "23183827\n",
      "23209657\n",
      "23216645\n",
      "23217712\n",
      "23236467\n",
      "23253866\n",
      "23263555\n",
      "23275563\n",
      "23284848\n",
      "23316280\n",
      "23332754\n",
      "23353684\n",
      "23353889\n",
      "23369005\n",
      "23369981\n",
      "23395900\n",
      "23395907\n",
      "23399914\n",
      "23403925\n",
      "23405092\n",
      "23414517\n",
      "23431397\n",
      "23449449\n",
      "23455607\n",
      "23463101\n",
      "23467085\n",
      "23505436\n",
      "23511972\n",
      "23514585\n",
      "23520446\n",
      "23533724\n",
      "23549287\n",
      "23549480\n",
      "23555304\n",
      "23565095\n",
      "23582324\n",
      "23582331\n",
      "23585889\n",
      "23593007\n",
      "23621612\n",
      "23622247\n",
      "23634843\n",
      "23650535\n",
      "23658700\n",
      "23667408\n",
      "23675303\n",
      "23680104\n",
      "23685356\n",
      "23693014\n",
      "23706742\n",
      "23708798\n",
      "23725059\n",
      "23734815\n",
      "23737971\n",
      "23741051\n",
      "23750211\n",
      "23752268\n",
      "23758976\n",
      "23772379\n",
      "23773523\n",
      "23782464\n",
      "23788678\n",
      "23799140\n",
      "23799367\n",
      "23823123\n",
      "23829672\n",
      "23840630\n",
      "23840749\n",
      "23840900\n",
      "23855374\n",
      "23857585\n",
      "23861867\n",
      "23866081\n",
      "23890821\n",
      "23902751\n",
      "23907583\n",
      "23909438\n",
      "23910724\n",
      "2391361\n",
      "23918937\n",
      "23933751\n",
      "23935490\n",
      "23935497\n",
      "23940795\n",
      "23948297\n",
      "23949442\n",
      "23967200\n",
      "23979715\n",
      "24001151\n",
      "24006493\n",
      "24009510\n",
      "24009866\n",
      "24034246\n",
      "24035192\n",
      "24056303\n",
      "24063750\n",
      "24065129\n",
      "24069158\n",
      "24069330\n",
      "24069433\n",
      "24075010\n",
      "24076655\n",
      "24076656\n",
      "24083380\n",
      "24086303\n",
      "24090070\n",
      "24094005\n",
      "24098548\n",
      "24113872\n",
      "24117850\n",
      "24125847\n",
      "24145797\n",
      "24161670\n",
      "24167781\n",
      "24176932\n",
      "24189400\n",
      "24191246\n",
      "24223725\n",
      "24240174\n",
      "24243021\n",
      "24244371\n",
      "24263861\n",
      "24269683\n",
      "24274578\n",
      "24275654\n",
      "24282027\n",
      "24286120\n",
      "24311597\n",
      "24314029\n",
      "24330623\n",
      "24344185\n",
      "24349196\n",
      "24349490\n",
      "24365180\n",
      "24366813\n",
      "24374083\n",
      "24397932\n",
      "24416391\n",
      "24434184\n",
      "24473148\n",
      "24502362\n",
      "24515439\n",
      "24527098\n",
      "24555568\n",
      "24561554\n",
      "24563863\n",
      "24566989\n",
      "24568222\n",
      "24582333\n",
      "24587342\n",
      "24610369\n",
      "24618038\n",
      "24618592\n",
      "24626987\n",
      "24643253\n",
      "24651726\n",
      "24656813\n",
      "24722491\n",
      "24754922\n",
      "24798445\n",
      "24823443\n",
      "24835508\n",
      "24843023\n",
      "24847877\n",
      "24855951\n",
      "24872509\n",
      "24879895\n",
      "24904275\n",
      "24914955\n",
      "24937146\n",
      "24960027\n",
      "24960071\n",
      "24963139\n",
      "24964212\n",
      "24983867\n",
      "25009464\n",
      "25147953\n",
      "25159688\n",
      "25170085\n",
      "25171412\n",
      "25225338\n",
      "25260594\n",
      "25260751\n",
      "25277244\n",
      "25278935\n",
      "25294836\n",
      "25294943\n",
      "25314077\n",
      "25321483\n",
      "25360523\n",
      "25374563\n",
      "25425574\n",
      "25445562\n",
      "25473596\n",
      "25519916\n",
      "25533335\n",
      "25609649\n",
      "25653167\n",
      "25697406\n",
      "25767811\n",
      "7528772\n",
      "7561682\n",
      "7593161\n",
      "7807015\n",
      "7844150\n",
      "8253836\n",
      "8551220\n",
      "8609167\n",
      "8627166\n",
      "8627180\n",
      "8666671\n",
      "8666672\n",
      "8691146\n",
      "8691154\n",
      "8707857\n",
      "8858162\n",
      "8922390\n",
      "9060478\n",
      "9128257\n",
      "9151673\n",
      "9214383\n",
      "9214386\n",
      "9334338\n",
      "9412461\n",
      "9425168\n",
      "9472029\n",
      "9531549\n",
      "9531566\n",
      "9628892\n",
      "9660868\n",
      "9700171\n",
      "9763420\n",
      "9786960\n",
      "9813092\n",
      "9817749\n",
      "9864353\n",
      "9864360\n",
      "9922454\n",
      "9971739\n"
     ]
    }
   ],
   "source": [
    "tsv_dir = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/scidt_fries_bioc_tsv4\"\n",
    "sentence_dir = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/fig_sentences\"\n",
    "\n",
    "for root, dirs, files in os.walk(tsv_dir):\n",
    "    for file in files:    \n",
    "        if os.path.isfile(root+'/'+file) and file[-4:]=='.tsv' :\n",
    "            pmid = file[:-4]\n",
    "            if( pmid in pmids ):\n",
    "                print( pmid )\n",
    "                fig_tagged_sentences = retrieve_sentences_for_modeling(root+'/'+file, pmid)\n",
    "                for fig in fig_tagged_sentences.keys():\n",
    "                    out = open(sentence_dir+'/'+pmid+'_'+fig+'.txt', 'w')\n",
    "                    for sent_hash in fig_tagged_sentences[fig]:\n",
    "                        out.write(sent_hash['text'] + '\\n')\n",
    "                    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to simplify INTACT records from their standard XML into TSV format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def build_figure_extraction_patterns():\n",
    "    bf = \"\\s*f(igs|igs\\.|ig|ig\\.|igure|\\.|ig\\:){0,1}\"\n",
    "    d =  \"\\s*(\\d+\\s*[\\.\\;\\,]{0,1}\\s*[a-z]*)\\s*\\.{0,1}\\s*\"\n",
    "    d_split =  \"\\s*(\\d*)\\s*[\\.\\;\\,]{0,1}\\s*([a-z]*)\"\n",
    "    interval = \"\\s*(\\d+)([a-z]+)\\\\-([a-z]+)\"\n",
    "    pattHash = {} \n",
    "    \n",
    "    figPatt = []\n",
    "    pattHash['figPatt'] = figPatt\n",
    "    \n",
    "    # 0. No alphanumeric codes at all: 'Figure. 1; more text'\n",
    "    figPatt.append(re.compile(\"^\" + bf + d + \"$\"))         \n",
    "    figPatt.append(re.compile(\"^\" + bf + \"\\s*(\\d+\\s*[\\.\\;\\,]{0,1}\\s*[a-z]*)[\\,\\;\\.]{0,1}\\s*t\"))\n",
    "    figPatt.append(re.compile(\"^\" + bf + \"\\s*(\\d+\\s*[\\.\\;\\,]{0,1}\\s*[a-z]*)[\\,\\;\\.]{0,1}\\s*s\"))\n",
    "    figPatt.append(re.compile(\"^\" + bf + \"\\s*(\\d+\\s*[\\.\\;\\,]{0,1}\\s*[a-z]*)[\\,\\;\\.]{0,1}\\s+and\\s+s\"))\n",
    "    \n",
    "    # [1]\n",
    "    simplePatt = re.compile(\"^\" + d + \"$\");\n",
    "    pattHash['simplePatt'] = simplePatt\n",
    "    \n",
    "    # [2,4]    \n",
    "    space2Patt = re.compile(\"^\" + bf + d + \"\\s+\" + bf + d + \"$\");\n",
    "    pattHash['space2Patt'] = space2Patt\n",
    "\n",
    "    # [2,4,6]    \n",
    "    space3Patt = re.compile(\"^\"+bf+d+\"\\s+\"+bf+d+\"\\s+\"+bf+d+\"$\");\n",
    "    pattHash['space3Patt'] = space3Patt\n",
    "\n",
    "    # [2,4]\n",
    "    fullComma2Patt = re.compile(\"^\" + bf + d + \"[\\;\\,]\" + bf + d + \"$\")\n",
    "    pattHash['fullComma2Patt'] = fullComma2Patt\n",
    "    \n",
    "    # [2,3]\n",
    "    comma2Patt = re.compile(\"^\" + bf + d + \"[\\;\\,]\" + d + \"$\")\n",
    "    pattHash['comma2Patt'] = comma2Patt\n",
    "\n",
    "    # [1,2]\n",
    "    simpleComma2Patt = re.compile(\"^\" + d + \"[\\;\\,]\" + d + \"$\")\n",
    "    pattHash['simpleComma2Patt'] = simpleComma2Patt\n",
    "\n",
    "    # [2,3,4]\n",
    "    comma3Patt = re.compile(\"^\" + bf + d + \"[\\;\\,]\" + d + \"[\\;\\,]\" + d + \"$\");\n",
    "    pattHash['comma3Patt'] = comma3Patt\n",
    "    \n",
    "    # [1,2,3]\n",
    "    simpleComma3Patt = re.compile(\"^\" + d + \"[\\;\\,]\" + d + \"[\\;\\,]\" + d + \"$\");\n",
    "    pattHash['simpleComma3Patt'] = simpleComma3Patt\n",
    "\n",
    "    # [2,3,4,5]\n",
    "    comma4Patt = re.compile(\"^\"+bf+d+\"[\\;\\,]\"+d+\"[\\;\\,]\"+d+\"[\\;\\,]\"+d+\"$\");\n",
    "    pattHash['comma4Patt'] = comma4Patt\n",
    "\n",
    "    # [2,3,4,5,6]\n",
    "    comma5Patt = re.compile(\"^\"+bf+d+\"[\\;\\,]\"+d+\"[\\;\\,]\"+d+\"[\\;\\,]\"+d+\"[\\;\\,]\"+d+\"$\");\n",
    "    pattHash['comma5Patt'] = comma5Patt\n",
    "\n",
    "    # [1,2,3,4]\n",
    "    simpleComma4Patt = re.compile(\"^\"+d+\"[\\;\\,]\"+d+\"[\\;\\,]\"+d+\"[\\;\\,]\"+d+\"$\");\n",
    "    pattHash['simpleComma4Patt'] = simpleComma4Patt\n",
    "\n",
    "    # [2,3]\n",
    "    and2Patt = re.compile(\"^\" + bf + d + \"\\s+and\\s+\" + d + \"$\");\n",
    "    pattHash['and2Patt'] = and2Patt\n",
    "    \n",
    "    # [1,2]\n",
    "    simpleAnd2Patt = re.compile(\"^\" + d + \"\\s+and\\s+\" + d + \"$\");\n",
    "    pattHash['simpleAnd2Patt'] = simpleAnd2Patt\n",
    "\n",
    "    # [1,2,3]\n",
    "    simple_a_and_b_patt = re.compile(\"^\" + d_split + \"\\s+and\\s+([a-z])$\");\n",
    "    pattHash['simple_a_and_b_patt'] = simple_a_and_b_patt\n",
    "\n",
    "    # [2,3,4]\n",
    "    a_and_b_patt = re.compile(\"^\" + bf + d_split + \"\\s+and\\s+([a-z])$\");\n",
    "    pattHash['a_and_b_patt'] = a_and_b_patt\n",
    "\n",
    "    # [1,2,3]\n",
    "    simple_a_comma_b_patt = re.compile(\"^\" + d_split + \"[\\;\\,]\\s*([a-z])$\");\n",
    "    pattHash['simple_a_comma_b_patt'] = simple_a_comma_b_patt\n",
    "\n",
    "    # [2,3,4]\n",
    "    a_comma_b_patt = re.compile(\"^\"+bf+d_split+\"[\\;\\,]\\s*([a-z])$\");\n",
    "    pattHash['a_comma_b_patt'] = a_comma_b_patt\n",
    "\n",
    "    # [1,2,3]\n",
    "    simple_a_comma_b_comma_c_patt = re.compile(\"^\" + d_split + \"[\\;\\,]\\s*([a-z])\\s*[\\;\\,]\\s*([a-z])$\");\n",
    "    pattHash['simple_a_comma_b_comma_c_patt'] = simple_a_comma_b_comma_c_patt\n",
    "\n",
    "    # [2,3,4]\n",
    "    a_comma_b_comma_c_patt = re.compile(\"^\"+bf+d_split+\"[\\;\\,]\\s*([a-z])\\s*[\\;\\,]\\s*([a-z])$\");\n",
    "    pattHash['a_comma_b_comma_c_patt'] = a_comma_b_comma_c_patt\n",
    "\n",
    "    # [2,3,4,5]\n",
    "    a_b_and_c_patt = re.compile(\"^\" + bf + d_split + \"[\\;\\,]\\s+([a-z])\\s+and\\s+([a-z])$\");\n",
    "    pattHash['a_b_and_c_patt'] = a_b_and_c_patt\n",
    "\n",
    "    # [1,2,3,4]\n",
    "    simple_a_b_and_c_patt = re.compile(\"^\" + d_split + \"[\\;\\,]\\s+([a-z])\\s+and\\s+([a-z])$\");\n",
    "    pattHash['simple_a_b_and_c_patt'] = simple_a_b_and_c_patt\n",
    "\n",
    "    tableFigPatt = re.compile(\"^t(ab\\.|ab|able){0,1}.*\" + bf + d + \"$\");\n",
    "    pattHash['tableFigPatt'] = tableFigPatt\n",
    "\n",
    "    intervalPatt = re.compile(\"^\" + bf + interval + \"$\");\n",
    "    pattHash['intervalPatt'] = intervalPatt\n",
    "\n",
    "    # simple single table (table 1, t1, tab. 1a)\n",
    "    # returned value is second group\n",
    "    tablePatt = re.compile(\"^t(ab\\.|ab|able){0,1}\\s*([\\di]+[a-z]{0,1})[\\,\\;\\.]{0,1}$\");\n",
    "    pattHash['tablePatt'] = tablePatt\n",
    "\n",
    "    # simple single table (table 1, t1, tab. 1a)\n",
    "    # returned value is third group\n",
    "    suppTablePatt = re.compile(\"^s(upp|upp.|lementary){0,1}\\s*t(ab\\.|ab|able){0,1}\\s*([i\\d]+[a-z]{0,1})[\\,\\;\\.]{0,1}$\");\n",
    "    pattHash['suppTablePatt'] = suppTablePatt\n",
    "    \n",
    "    return pattHash\n",
    "\n",
    "def run_simple_matcher(fig_text, patt_hash, patt_code, groups=[1]):\n",
    "    match = re.search(patt_hash.get(patt_code), fig_text)\n",
    "    results = []\n",
    "    if( match ) :\n",
    "        for g in groups:\n",
    "            results.append(match.group(g))\n",
    "        return results\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def build_matched_string(matched_list,code):\n",
    "    matched_str = \"\"\n",
    "    for mf in matched_list:\n",
    "        if len(matched_str) > 0 :\n",
    "            matched_str += '|'\n",
    "        matched_str += code + mf.replace(\" \", \"\").replace(\".\", \"\")\n",
    "    return matched_str\n",
    "\n",
    "def run_matcher(fig_text, patt_hash):\n",
    "    \n",
    "    if(fig_text == 'nfa' ):\n",
    "        return None\n",
    "    \n",
    "    # strip out all parentheses.\n",
    "    paren_patt = re.compile(\"(\\(.+?\\))\")\n",
    "    fig_text = re.sub(paren_patt, \"\", fig_text)\n",
    "\n",
    "    # covert & to 'and'.\n",
    "    fig_text = fig_text.replace(\"&\", \"and\")\n",
    "    \n",
    "    fig_patt = patt_hash.get('figPatt')\n",
    "    for p in fig_patt:\n",
    "        match = re.search(p, fig_text)\n",
    "        if match:\n",
    "            return 'f' + match.group(2).replace(\" \",\"\").replace(\".\",\"\").replace(\",\",\"\")\n",
    "    \n",
    "    # [1] simplePatt\n",
    "    # [2,4] space2Patt\n",
    "    # [2,4,6] space3Patt\n",
    "    # [2,4] fullComma2Patt\n",
    "    # [2,3] comma2Patt\n",
    "    # [1,2] simpleComma2Patt\n",
    "    # [2,3,4] comma3Patt \n",
    "    # [1,2,3] simpleComma3Patt\n",
    "    # [2,3,4,5] comma4Patt\n",
    "    # [1,2,3,4] simpleComma4Patt\n",
    "    # [1,2] simpleAnd2Patt\n",
    "    # [1,2,3] simple_a_comma_b_patt \n",
    "    # [2,3,4] a_comma_b_patt \n",
    "    # [2,3,4,5]   a_b_and_c_patt \n",
    "    # [1,2,3,4] simple_a_b_and_c_patt\n",
    "    \n",
    "    matched_figs = run_simple_matcher(fig_text, patt_hash, 'simplePatt', [1])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'tableFigPatt', [3])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'comma2Patt', [2,3])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'fullComma2Patt', [2,4])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'simpleComma2Patt', [1,2])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'comma3Patt', [2,3,4])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'simpleComma3Patt', [1,2,3])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'comma4Patt', [2,3,4,5])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'simpleComma4Patt', [1,2,3,4])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'comma5Patt', [2,3,4,5,6])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'space2Patt', [2,4])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'space3Patt', [2,4,6])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'simpleAnd2Patt', [1,2])\n",
    "    if( matched_figs is None ):\n",
    "        matched_figs = run_simple_matcher(fig_text, patt_hash, 'and2Patt', [2,3])\n",
    "    if( matched_figs is None ):\n",
    "        match = re.search(patt_hash.get('simple_a_comma_b_patt'), fig_text)\n",
    "        if( match ):\n",
    "            f =  match.group(1)\n",
    "            a = match.group(2)\n",
    "            b = match.group(3)\n",
    "            return 'f'+f+a+'|'+'f'+f+b\n",
    "    if( matched_figs is None ):\n",
    "        match = re.search(patt_hash.get('a_comma_b_patt'), fig_text)\n",
    "        if( match ):\n",
    "            f =  match.group(2)\n",
    "            a = match.group(3)\n",
    "            b = match.group(4)\n",
    "            return 'f'+f+a+'|'+'f'+f+b\n",
    "    if( matched_figs is None ):\n",
    "        match = re.search(patt_hash.get('simple_a_and_b_patt'), fig_text)\n",
    "        if( match ):\n",
    "            f =  match.group(1)\n",
    "            a = match.group(2)\n",
    "            b = match.group(3)\n",
    "            return 'f'+f+a+'|'+'f'+f+b\n",
    "    if( matched_figs is None ):\n",
    "        match = re.search(patt_hash.get('a_and_b_patt'), fig_text)\n",
    "        if( match ):\n",
    "            f =  match.group(2)\n",
    "            a = match.group(3)\n",
    "            b = match.group(4)\n",
    "            return 'f'+f+a+'|'+'f'+f+b\n",
    "    if( matched_figs is None ):\n",
    "        match = re.search(patt_hash.get('a_b_and_c_patt'), fig_text)\n",
    "        if( match ):\n",
    "            f =  match.group(2)\n",
    "            a = match.group(3)\n",
    "            b = match.group(4)\n",
    "            c = match.group(5)\n",
    "            return 'f'+f+a+'|'+'f'+f+b+'|'+'f'+f+c\n",
    "    if( matched_figs is None ):\n",
    "        match = re.search(patt_hash.get('simple_a_b_and_c_patt'), fig_text)\n",
    "        if( match ):\n",
    "            f =  match.group(1)\n",
    "            a = match.group(2)\n",
    "            b = match.group(3)\n",
    "            c = match.group(4)\n",
    "            return 'f'+f+a+'|'+'f'+f+b+'|'+'f'+f+c\n",
    "    if( matched_figs is None ):\n",
    "        match = re.search(patt_hash.get('simple_a_comma_b_comma_c_patt'), fig_text)\n",
    "        if( match ):\n",
    "            f =  match.group(1)\n",
    "            a = match.group(2)\n",
    "            b = match.group(3)\n",
    "            c = match.group(4)\n",
    "            return 'f'+f+a+'|'+'f'+f+b+'|'+'f'+f+c\n",
    "    if( matched_figs is None ):\n",
    "        match = re.search(patt_hash.get('a_comma_b_comma_c_patt'), fig_text)\n",
    "        if( match ):\n",
    "            f =  match.group(2)\n",
    "            a = match.group(3)\n",
    "            b = match.group(4)\n",
    "            c = match.group(5)\n",
    "            return 'f'+f+a+'|'+'f'+f+b+'|'+'f'+f+c\n",
    "    if( matched_figs is None ):\n",
    "        match = re.search(patt_hash.get('intervalPatt'), fig_text)\n",
    "        if( match ):\n",
    "            fig_number =  match.group(2)\n",
    "            start = match.group(3)\n",
    "            end = match.group(4)\n",
    "            if( len(start) > 1 or len(end)>1 ):\n",
    "                return None\n",
    "            matched_str = \"\"\n",
    "            subfigs = [chr(i) for i in range(ord(start),ord(end)+1)] \n",
    "            for subfig in subfigs :\n",
    "                if len(matched_str) > 0 :\n",
    "                    matched_str += '|'\n",
    "                matched_str += 'f' + fig_number + subfig\n",
    "            return matched_str\n",
    "            \n",
    "    if(matched_figs is not None):\n",
    "        return build_matched_string(matched_figs, 'f')\n",
    "    \n",
    "    matched_tab = run_simple_matcher(fig_text, patt_hash, 'tablePatt', [2])\n",
    "    if(matched_tab is not None):\n",
    "        return build_matched_string(matched_tab, 't')\n",
    "\n",
    "    matched_tab = run_simple_matcher(fig_text, patt_hash, 'suppTablePatt', [3])\n",
    "    if(matched_tab is not None):\n",
    "        return build_matched_string(matched_tab, 'st')\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_simple_intact_data(input, title, tsv_output):\n",
    "    \n",
    "    with open(input, 'r') as input_file:\n",
    "        xml = input_file.read()\n",
    "        \n",
    "    # Check if the figure legends are specified\n",
    "    if \"\\\"figure legend\\\"\" not in xml: \n",
    "        return  \n",
    "    \n",
    "    soup = BeautifulSoup(xml, 'lxml')    \n",
    "\n",
    "    intact_headings = ['pmid','i_id','orig_fig','fig','type','type_xref','p1_name',\n",
    "                       'p1_xref','p1_site','p2_name','p2_xref','p2_site','p3_name',\n",
    "                       'p3_xref','p3_site','i_meth','p_meth']\n",
    "    intact_rows = []\n",
    "\n",
    "    patt_hash = build_figure_extraction_patterns()\n",
    "\n",
    "    # EXPERIMENTS\n",
    "    all_expt_dict = {}\n",
    "    for e in soup.select('experimentlist experimentdescription'):\n",
    "        ex_dict = {}\n",
    "        ex_dict['i_meth'] = e.interactiondetectionmethod.names.shortlabel.text\n",
    "        ex_dict['p_meth'] = e.participantidentificationmethod.names.shortlabel.text \n",
    "        all_expt_dict[e.get('id')] = ex_dict\n",
    "\n",
    "    # INTERACTORS\n",
    "    all_int_dict = {}\n",
    "    for i1 in soup.select('interactorlist interactor'):\n",
    "        int_dict = {}\n",
    "        int_dict['name'] = i1.names.shortlabel.text\n",
    "        urls = []\n",
    "        for t in i1.select('primaryref[db=\"uniprotkb\"]'):\n",
    "            if( t.get('reftype') == 'identity' ) :\n",
    "                urls.append(t.get('id'))\n",
    "        for t in i1.select('secondaryref[db=\"uniprotkb\"]'):\n",
    "            if( t.get('reftype') == 'identity' ) :\n",
    "                urls.append(t.get('id'))\n",
    "        int_dict['xref'] = urls\n",
    "        all_int_dict[i1.get('id')] = int_dict\n",
    "\n",
    "    # INTERACTIONS\n",
    "    for i in soup.select('interactionlist interaction'):\n",
    "        int_dict = {}\n",
    "        int_dict['pmid'] = title\n",
    "        int_dict['i_id'] = i.get('id')\n",
    "        int_dict['type'] = i.interactiontype.names.shortlabel.text        \n",
    "        int_dict['type_xref'] = i.interactiontype.xref.primaryref.get('id')\n",
    "        p_count = 1\n",
    "        for p_tag in i.select('participantlist participant'):\n",
    "            p_id = p_tag.interactorref.text\n",
    "            p = all_int_dict[p_id]\n",
    "            int_dict['p'+str(p_count)+\"_name\"] = p.get('name')\n",
    "            int_dict['p'+str(p_count)+\"_xref\"] = '|'.join(p.get('xref'))\n",
    "            p_count += 1\n",
    "        int_dict['fig'] = '-'\n",
    "        for a in i.select('attributelist attribute[name]'):\n",
    "            if( a.get('name') == \"figure legend\" ):\n",
    "                fig_text = a.text.lower()\n",
    "                fig_text = run_matcher(fig_text, patt_hash)\n",
    "                if( fig_text is None):\n",
    "                    print(a.text.lower() + \"  :  None\")\n",
    "                int_dict['orig_fig'] = a.text\n",
    "                int_dict['fig'] = fig_text\n",
    "        e_id = i.experimentlist.experimentref.text\n",
    "        e = all_expt_dict.get(e_id)\n",
    "        if( e is not None ):\n",
    "            int_dict['i_meth'] = e.get('i_meth', '-')\n",
    "            int_dict['p_meth'] = e.get('p_meth', '-')\n",
    "        else: \n",
    "            int_dict['i_meth'] = '-'\n",
    "            int_dict['p_meth'] = '-'\n",
    "            \n",
    "        r = []\n",
    "        for h in intact_headings:\n",
    "            r.append(int_dict.get(h,'-'))\n",
    "        intact_rows.append(r)\n",
    "        \n",
    "    intact_df = pd.DataFrame.from_records(intact_rows, columns=intact_headings) \n",
    "    intact_df.to_csv(tsv_output, sep='\\t', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution of code to simplify INTACT records from standard XML into TSV format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/simple_intact_files/\n"
     ]
    }
   ],
   "source": [
    "stem = '/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/'\n",
    "intact_dir = stem + 'gold_standard/'\n",
    "simple_intact_dir = stem + 'simple_intact_files/'\n",
    "\n",
    "print(simple_intact_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in os.walk(intact_dir):\n",
    "    for infile in glob(os.path.join(x[0], '*.xml')):\n",
    "        fn = ntpath.basename(infile)\n",
    "        if( os.path.isfile(infile) and fn.endswith('.xml') ):\n",
    "            title = fn.replace(\".xml\", \"\")\n",
    "            if( title not in pmids ):\n",
    "                continue\n",
    "\n",
    "            outfile = simple_intact_dir + \"/\" + title + \".tsv\"\n",
    "            if( not os.path.isfile(outfile) ):\n",
    "                try:\n",
    "                    extract_simple_intact_data(infile, title, outfile)\n",
    "                except KeyError:\n",
    "                    print(\"KeyError for \" + infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script to convert collections of PSI-MI2.5 files to biopax. We've updated the script to run our updated PaxTools from github.com/BMKEG/Paxtools which includes annotations about Figures in Biopax evidence codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paxtools_jar = \"/Users/Gully/Coding/git/biopax/Paxtools/paxtools-console/target/paxtools.jar\"\n",
    "\n",
    "data_dir = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/gold_standard_data\"\n",
    "open_access_dir = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/oa_gold_data\"\n",
    "biopax_dir = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/biopax\"\n",
    "new_biopax_dir = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/biopax_reformat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THIS RUNS THE UPDATED PAXTOOLS TO GENERATE BIOPAX 3 DATA FOR OUR USE.\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for file in files:    \n",
    "        if os.path.isfile(root+'/'+file) and file[-4:]=='.xml' :\n",
    "            pmid = file[:-4]\n",
    "\n",
    "            if( pmid in pmids ): \n",
    "                cmds = [\"java\",\"-jar\",paxtools_jar,\"toLevel3\",root+'/'+file,biopax_dir+'/'+pmid+'_biopax.xml','-psimiToComplexes']\n",
    "                print \" \".join(cmds)\n",
    "                call(cmds)\n",
    "                print \"\\tDONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reformat_figure_legend_annotations(input):\n",
    "    \n",
    "    with open(input, 'r') as input_file:\n",
    "        xml = input_file.read()\n",
    "        \n",
    "    if \">Figure:\" not in xml: \n",
    "        return  \n",
    "\n",
    "    patt_hash = build_figure_extraction_patterns()\n",
    "    fig_patt = re.compile(\">Figure:(.*?)<\")\n",
    "\n",
    "    output = \"\"\n",
    "    with open(input, 'r') as input_file:\n",
    "        for line in input_file.readlines(): \n",
    "            match = re.search(fig_patt, line)\n",
    "            if match: \n",
    "                fig_text = match.group(1).lower()\n",
    "                new_fig_text = run_matcher(fig_text, patt_hash)\n",
    "                if( new_fig_text is not None ):\n",
    "                    line = re.sub(fig_patt,\">Figure:\"+new_fig_text+\"</bp\",line)\n",
    "                    #print fig_text + '==>' + new_fig_text\n",
    "\n",
    "            output += line\n",
    "\n",
    "    return output\n",
    "\n",
    "# THIS FORMATS FIGURE ANNOTATIONS IN THE UPDATED BIOPAX 3 FILES.\n",
    "for root, dirs, files in os.walk(biopax_dir):\n",
    "    for file in files:    \n",
    "        if os.path.isfile(root+'/'+file) and file[-4:]=='.xml' :\n",
    "            # Now, load each BIOPAX 3 file, and run the patterns on text found in the XML\n",
    "            reformatted_text = reformat_figure_legend_annotations(root+'/'+file)\n",
    "\n",
    "            if reformatted_text is not None:\n",
    "                with open(new_biopax_dir+'/'+file, 'w') as output_file:\n",
    "                    output_file.write(reformatted_text)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to find which pmids have intact records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def copy_figure_files(intactFile, figAssigmentDir, outDir):\n",
    "\n",
    "    frames = []\n",
    "  \n",
    "    intact_tsv = pd.read_csv(intactFile, sep='\\t')\n",
    "    \n",
    "    fries_sentences = []\n",
    "    fries_hits = []\n",
    "    fries_events = []\n",
    "    count = 0\n",
    "    fries_count = 0\n",
    "    hit_count = 0\n",
    "    miss_count = 0\n",
    "    for i,row in intact_tsv.iterrows():\n",
    "        pmid = str(row['pmid'])\n",
    "        fig = str(row['fig'])\n",
    "        src_file = figAssigmentDir+'/'+pmid+'_'+fig+'.txt'\n",
    "        dst_file = outDir+'/'+pmid+'_'+fig+'.txt'\n",
    "        if( os.path.isfile(figAssigmentDir + '/'+pmid+'_'+fig+'.txt') ) :\n",
    "            copyfile(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig_sentences_dir = stem + 'fig_sentences'\n",
    "out_sentences_dir = stem + 'fig_sentences_in_intact'\n",
    "\n",
    "for root, dirs, files in os.walk(simple_intact_dir):\n",
    "    for file in files:    \n",
    "        if os.path.isfile(root+'/'+file) and file[-4:]=='.tsv' :\n",
    "            copy_figure_files(root+'/'+file, fig_sentences_dir, out_sentences_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to link the intact files to the sciDt data.\n",
    "\n",
    "This is derived from the simplified TSV-format generated above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def link_scidt_to_intact(intactFile, scidtDir, outFile):\n",
    "\n",
    "    frames = []\n",
    "  \n",
    "    intact_tsv = pd.read_csv(intactFile, sep='\\t')\n",
    "    \n",
    "    fries_sentences = []\n",
    "    fries_hits = []\n",
    "    fries_events = []\n",
    "    count = 0\n",
    "    fries_count = 0\n",
    "    hit_count = 0\n",
    "    miss_count = 0\n",
    "    for i,row in intact_tsv.iterrows():\n",
    "        pmid = row['pmid']\n",
    "        print(pmid)\n",
    "        intact_fig = row['fig']\n",
    "        p1 = row['p1_xref']\n",
    "        p2 = row['p2_xref']\n",
    "        p3 = row['p3_xref']\n",
    "\n",
    "        fries_events_local = []\n",
    "        \n",
    "        # find the figure numbers in the paper designation \n",
    "        scidt_path = os.path.join(scidtDir, str(pmid) + \".tsv\")\n",
    "        if( os.path.isfile( scidt_path ) ):\n",
    "            scidt_tsv = pd.read_csv(scidt_path, sep='\\t')\n",
    "            for i2,row2 in scidt_tsv.iterrows():\n",
    "                fries_sentence = row2['friesSentenceId'] \n",
    "                fries_event = row2['friesEventsTypes'] \n",
    "                scidt_figs = row2['Figure Assignment']\n",
    "                if( scidt_figs == scidt_figs and fries_event == fries_event):\n",
    "                    for scidt_fig in scidt_figs.split('|'):\n",
    "                        if scidt_fig == intact_fig and 'complex-assembly' in fries_event:\n",
    "                            fries_count += 1\n",
    "                            if( p1 != p1 or p2 != p2 or p3 != p3):\n",
    "                                hit = \"MISS\"\n",
    "                                miss_count += 1\n",
    "                            elif( (p1 == '-' or p1 in fries_event) and \n",
    "                                (p2 == '-' or p2 in fries_event) and \n",
    "                                (p3 == '-' or p3 in fries_event) ):\n",
    "                                hit = \"HIT\"\n",
    "                                hit_count += 1\n",
    "                            else :\n",
    "                                hit = \"MISS\"\n",
    "                                miss_count += 1\n",
    "                            fries_events_local.append(fries_event + '[' + hit + ']')\n",
    "                            \n",
    "        fries_events.append(fries_events_local)\n",
    "    \n",
    "    intact_tsv['fries_events'] = pd.Series(fries_events)\n",
    "        \n",
    "    intact_tsv.to_csv(outFile, sep='\\t')\n",
    "    print (\"COUNT: %d\" % fries_count)\n",
    "    print (\"HITS: %d\" % hit_count)\n",
    "    print (\"MISSES: %d\" % miss_count )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run through Biopax entries. Load each file and search for evidence. Link that evidence to sentences via figure legends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsv_dir = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/scidt_fries_bioc_tsv4\"\n",
    "new_biopax_dir = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/biopax_reformat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "9\n",
      "1\n",
      "2\n",
      "1\n",
      "10\n",
      "5\n",
      "4\n",
      "5\n",
      "1\n",
      "9\n",
      "5\n",
      "18\n",
      "1\n",
      "6\n",
      "13\n",
      "9\n",
      "21\n",
      "215\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "6\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "9\n",
      "6\n",
      "5\n",
      "9\n",
      "4\n",
      "1\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "15\n",
      "8\n",
      "14\n",
      "13\n",
      "6\n",
      "3\n",
      "1\n",
      "4\n",
      "4\n",
      "29\n",
      "11\n",
      "24\n",
      "14\n",
      "5\n",
      "2\n",
      "11\n",
      "7\n",
      "14\n",
      "2\n",
      "6\n",
      "5\n",
      "3\n",
      "2\n",
      "6\n",
      "5\n",
      "1\n",
      "5\n",
      "3\n",
      "1\n",
      "12\n",
      "5\n",
      "6\n",
      "1\n",
      "7\n",
      "13\n",
      "2\n",
      "6\n",
      "2\n",
      "6\n",
      "8\n",
      "4\n",
      "1"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def generate_annotation_page(pmid, biopax_path, scidt_path):\n",
    "    annotation_items = []\n",
    "    annotation_page = {\n",
    "        \"@context\": \"http://www.w3.org/ns/anno.jsonld\",\n",
    "        \"id\": \"http://sciknowengine.isi.edu/iswc17/annotation_page/\"+pmid,\n",
    "        \"type\": \"AnnotationPage\",\n",
    "        \"partOf\": {\n",
    "            \"id\": \"http://sciknowengine.isi.edu/iswc17/annotations\"\n",
    "        },\n",
    "        \"next\": \"http://example.org/page2\",\n",
    "        \"startIndex\": 0,\n",
    "        \"items\": annotation_items\n",
    "    }\n",
    "    \n",
    "    biopax_lines = []\n",
    "    with open(biopax_path, 'r') as biopax_file:\n",
    "        biopax_lines = biopax_file.readlines()\n",
    "\n",
    "    scidt_tsv = pd.read_csv(scidt_path, sep='\\t')\n",
    "    \n",
    "    we_are_on = False\n",
    "    evidence_patt = re.compile(\"<bp:Evidence rdf:about\\=\\\"(.*?)\\\">\")\n",
    "    figure_patt = re.compile(\">Figure:(.*?)<\")\n",
    "    evidence_off_patt = re.compile(\"<\\/bp:Evidence>\")\n",
    "\n",
    "    evidence_code = ''\n",
    "    figure_code = ''\n",
    "    for biopax_line in biopax_lines: \n",
    "        evidence_match = re.search(evidence_patt, biopax_line)\n",
    "        if evidence_match: \n",
    "            evidence_code = evidence_match.group(1)\n",
    "            figure_code = ''\n",
    "            we_are_on = True\n",
    "\n",
    "        figure_match = re.search(figure_patt, biopax_line)\n",
    "        if figure_match: \n",
    "            figure_code = figure_match.group(1)\n",
    "                            \n",
    "        if we_are_on and len(figure_code)>0:\n",
    "            \n",
    "            targets = []\n",
    "            annotation = {\n",
    "                \"id\": \"http://sciknowengine.isi.edu/iswc17/annotations/\"+pmid+'#'+str(count),\n",
    "                \"type\": \"Annotation\",\n",
    "                \"body\": {\n",
    "                    \"id\": evidence_code,\n",
    "                    \"type\": \"Dataset\"\n",
    "                },\n",
    "                \"target\": targets\n",
    "            }\n",
    "            annotation_items.append(annotation)\n",
    "            \n",
    "            for i, row in scidt_tsv.iterrows():\n",
    "                sid = row['SentenceId']\n",
    "                text = row['Sentence Text']\n",
    "                codeStr = row['Codes']\n",
    "                expts = row['ExperimentValues']\n",
    "                paragraph = row['Paragraph']\n",
    "                heading = row['Headings']\n",
    "                discourse = row['Discourse Type']\n",
    "                offset_start = row['Offset_Begin']\n",
    "                offset_end = row['Offset_End']\n",
    "                fig = row['Figure Assignment']\n",
    "                \n",
    "                if(fig != fig):\n",
    "                    continue\n",
    "                \n",
    "                for f in re.split(\"|\", fig):\n",
    "                    if( f in figure_code):\n",
    "                        \n",
    "                        targets.append({\n",
    "                            \"source\": \"https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/\" + str(pmid),\n",
    "                            \"selector\": [{\n",
    "                                    \"type\": \"TextQuoteSelector\",\n",
    "                                    \"exact\": text\n",
    "                                },\n",
    "                                {\n",
    "                                    \"type\": \"TextPositionSelector\",\n",
    "                                    \"start\": offset_start,\n",
    "                                    \"end\": offset_end\n",
    "                                }]\n",
    "                        })\n",
    "            \n",
    "            annotation['target'] = targets\n",
    "            we_are_on = False\n",
    "        \n",
    "    #print len(annotation_items)\n",
    "    annotation_page['items'] = annotation_items\n",
    "                \n",
    "    return annotation_page\n",
    "\n",
    "annotation_collection_path = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/annotation_collection.json\"\n",
    "annotation_pages_path = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/pages\"\n",
    "\n",
    "page = {}\n",
    "annotation_collection = {\n",
    "  \"@context\": \"http://www.w3.org/ns/anno.jsonld\",\n",
    "  \"id\": \"http://sciknowengine.isi.edu/iswc17/annotations\",\n",
    "  \"type\": \"AnnotationCollection\",\n",
    "  \"label\": \"Anntoations linking BioPax records from the INTACT database to text fragments describing evidence\",\n",
    "  \"total\": 0,\n",
    "  \"first\": page\n",
    "}\n",
    "\n",
    "count = 0\n",
    "annotation_pages = {}\n",
    "last_annotation_page = None\n",
    "for root, dirs, files in os.walk(new_biopax_dir):\n",
    "    for file in files:    \n",
    "        if os.path.isfile(root+'/'+file) and file[-4:]=='.xml' :\n",
    "            l = len('_biopax.xml')\n",
    "            pmid = file[:-l]\n",
    "            tsv_file = tsv_dir+'/'+str(pmid)+'.tsv'\n",
    "            \n",
    "            if not os.path.isfile(tsv_file):\n",
    "                continue\n",
    "                \n",
    "            annotation_page = generate_annotation_page(pmid, root+'/'+file, tsv_dir+'/'+str(pmid)+'.tsv')\n",
    "            count += 1\n",
    "            #print json.dumps(annotation_page, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "            \n",
    "            if(last_annotation_page is None):\n",
    "                annotation_collection['first'] = annotation_page['id']\n",
    "            else:\n",
    "                last_annotation_page['next'] = annotation_page['id']\n",
    "            \n",
    "            annotation_page_dump = json.dumps(annotation_page, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "            with open(annotation_pages_path+'/page_'+pmid+'.json', 'w') as annotation_page_file:\n",
    "                annotation_page_file.write(annotation_page_dump)\n",
    "            \n",
    "            last_annotation_page = annotation_page\n",
    "            \n",
    "annotation_collection['total'] = count\n",
    "annotation_collection_dump = json.dumps(annotation_collection, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "with open(annotation_collection_path, 'w') as annotation_collection_file:\n",
    "    annotation_collection_file.write(annotation_collection_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotation_collection_path = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/annotation_collection.json\"\n",
    "annotation_pages_path = \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/pages\"\n",
    "\n",
    "annotation_collection_dump = json.dumps(annotation_collection, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "with open(annotation_collection_path, 'w') as annotation_collection_file:\n",
    "    annotation_collection_file.write(annotation_collection_dump)\n",
    "\n",
    "for pmid in annotation_pages.keys():\n",
    "    page = annotation_pages[pmid]\n",
    "    annotation_page_dump = json.dumps(page, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    with open(annotation_pages_path+'/page_'+pmid+'.json', 'w') as annotation_page_file:\n",
    "        annotation_page_file.write(annotation_page_dump)\n",
    "    \n",
    "print json.dumps(annotation_collection, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
